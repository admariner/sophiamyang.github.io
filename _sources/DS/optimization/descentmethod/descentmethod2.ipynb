{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descent method — Steepest descent and conjugate gradient in Python\n",
    "\n",
    "*Python implementation*\n",
    "\n",
    "Let’s start with this equation and we want to solve for x:\n",
    "\n",
    "$Ax = b $\n",
    "\n",
    "The solution x the minimize the function below when A is **symmetric positive definite** (otherwise, x could be the maximum). It is because the gradient of f(x), ∇f(x) = Ax- b. And when Ax=b, ∇f(x)=0 and thus x is the minimum of the function.\n",
    "\n",
    "$f(x) = \\frac{1}{2}x^TAx-x^Tb$\n",
    "\n",
    "In this article, I am going to show you two ways to find the solution x — method of Steepest Descent and method of Conjugate Gradient.\n",
    "\n",
    "## Method of Steepest Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def is_pos_def(x):\n",
    "    \"\"\"check if a matrix is symmetric positive definite\"\"\"\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def steepest_descent(A, b, x):\n",
    "    \"\"\"\n",
    "    Solve Ax = b\n",
    "    Parameter x: initial values\n",
    "    \"\"\"\n",
    "    if (is_pos_def(A) == False) | (A != A.T).any():\n",
    "        raise ValueError('Matrix A needs to be symmetric positive definite (SPD)')\n",
    "    r = b - A @ x\n",
    "    k = 0\n",
    "    while LA.norm(r) > 1e-10 :\n",
    "        p = r\n",
    "        q = A @ p\n",
    "        alpha = (p @ r) / (p @ q)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * q\n",
    "        k =+ 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use this `steepest_descent` function to calculate\n",
    "$$\\begin{bmatrix} 3 & 2 \\\\ 2 & 3 \\end{bmatrix} X = \\begin{bmatrix} -2 \\\\ 7 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3, 2], [2, 3]])\n",
    "b = np.array([-2, 7])\n",
    "x0 = np.array([0, 0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "steepest_descent(A, b, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the steepest_descent method, we get a value of (-4,5) and a wall time > 1 ms.\n",
    "\n",
    "## Conjugate gradient method in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b):\n",
    "    if (is_pos_def(A) == False) | (A != A.T).any():\n",
    "        raise ValueError('Matrix A needs to be symmetric positive definite (SPD)')\n",
    "    r = b \n",
    "    k = 0\n",
    "    x = np.zeros(A.shape[-1])\n",
    "    while LA.norm(r) > 1e-10 :\n",
    "        if k == 0:\n",
    "            p = r\n",
    "        else: \n",
    "            gamma = - (p @ A @ r)/(p @ A @ p)\n",
    "            p = r + gamma * p\n",
    "        alpha = (p @ r) / (p @ A @ p)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * (A @ p)\n",
    "        k =+ 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "conjugate_gradient(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conjugate_gradient function, we got the same value (-4, 5) and wall time < 400 μs, which is a lot faster than the steepest descent.\n",
    "\n",
    "## Visualizing steepest descent and conjugate gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import holoviews as hv\n",
    "hv.extension('plotly')\n",
    "\n",
    "def steepest_descent_store_result(A, b, x):\n",
    "    x_steps = [x]\n",
    "    y_steps = [0.5 * x @ A @ x - x @ b]\n",
    "    \n",
    "    if (is_pos_def(A) == False) | (A != A.T).any():\n",
    "        raise ValueError('Matrix A needs to be symmetric positive definite (SPD)')\n",
    "    r = b - A @ x\n",
    "    k = 0\n",
    "    while LA.norm(r) > 1e-10 :\n",
    "        p = r\n",
    "        q = A @ p\n",
    "        alpha = (p @ r) / (p @ q)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * q\n",
    "        k =+ 1\n",
    "        x_steps.append(x)\n",
    "        y_steps.append(0.5 * x @ A @ x - x @ b)\n",
    "\n",
    "    return x, x_steps, y_steps\n",
    "\n",
    "def conjugate_gradient_store_result(A, b):\n",
    "    if (is_pos_def(A) == False) | (A != A.T).any():\n",
    "        raise ValueError('Matrix A needs to be symmetric positive definite (SPD)')\n",
    "    r = b \n",
    "    k = 0\n",
    "    x = np.zeros(A.shape[-1])\n",
    "    x_steps = [x]\n",
    "    y_steps = [0.5 * x @ A @ x - x @ b]\n",
    "    while LA.norm(r) > 1e-10 :\n",
    "        if k == 0:\n",
    "            p = r\n",
    "        else: \n",
    "            gamma = - (p @ A @ r)/(p @ A @ p)\n",
    "            p = r + gamma * p\n",
    "        alpha = (p @ r) / (p @ A @ p)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * (A @ p)\n",
    "        k =+ 1\n",
    "        x_steps.append(x)\n",
    "        y_steps.append(0.5 * x @ A @ x - x @ b)\n",
    "\n",
    "    return x, x_steps, y_steps\n",
    "\n",
    "def viz_descent(x_steps, y_steps):\n",
    "    size = 50\n",
    "    x1s = np.linspace(-6, 6, size)\n",
    "    x2s = np.linspace(-6, 6, size)\n",
    "    x1, x2  = np.meshgrid(x1s, x2s)\n",
    "    Z = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            x = np.array([x1[i,j], x2[i,j]])\n",
    "            Z[i,j] = 0.5 * x @ A @ x - x @ b\n",
    "    surface = hv.Surface((x1s, x2s, Z)).opts(colorbar=True, width=700, height=700, cmap='Turbo',  alpha=.75)\n",
    "    points= np.concatenate([np.stack(x_steps), np.array(y_steps)[:, np.newaxis]], axis=1)\n",
    "    path = hv.Path3D(points).opts(width=700, height=700, color='black', line_width=1)\n",
    "    return surface * path\n",
    "\n",
    "# visualize steepest descent method\n",
    "_, x_steps, y_steps = steepest_descent_store_result(A, b, x0)\n",
    "viz_descent(x_steps, y_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize conjugate gradient method\n",
    "_, x_steps, y_steps = conjugate_gradient_store_result(A, b)\n",
    "viz_descent(x_steps, y_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoy this article. If you are interested in the math behind the two methods, check out this [article](https://sophiamyang.github.io/DS/optimization/descentmethod1.html).\n",
    "\n",
    "By Sophia Yang on [September 11, 2020](https://medium.com/dsc-msit/descent-method-steepest-descent-and-conjugate-gradient-in-python-85aa4c4aac7b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
