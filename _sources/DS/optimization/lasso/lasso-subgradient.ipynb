{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa000c52",
   "metadata": {},
   "source": [
    "# Lasso regression from¬†scratch\n",
    "*Subgradient method implementation in¬†Python*\n",
    "\n",
    "## Problem statement \n",
    "\n",
    "Let‚Äôs assume we have N people/observations, each person has M features. We are given:\n",
    "\n",
    "A matrix $ùëã$ is $\\mathbb{R}^{N\\times M}$. $X_{ij}$ represents person i with feature j.\n",
    "\n",
    "A vector $y$ is $\\mathbb{R}^{N}$. $y_{i}$ represents the output of person i.\n",
    "\n",
    "We expect y is a linear combination of features in X and we want to find out the relationship between X and y. If y is continous, we know that this is just a linear regression problem that tries to solve w for $y = Xw + b$. \n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "There are many approaches to solve the linear regression problem. The most basic approach Ordinary Least Squares tries to minimize $\\min_w \\|{Xw-y}\\|_2^2$. Ridge regression adds a L2 regularization term on the weights $\\min_w \\left[\\|{Xw-y}\\|_2^2 + \\alpha \\|{w}\\|_2^2\\right]$\n",
    "\n",
    "\n",
    "Lasso regression adds a L1 regularization term on the weights $\\min_w \\left[\\|{Xw-y}\\|_2^2 + \\alpha \\|{w}\\|_1 \\right]$. The first term $\\|{Xw-y}\\|_2^2$ is smooth, convex, and differentiable. The second term, however, is not smooth and not differentiable. Therefore, there are lots of fun methods for minimizing this issue. This article will walk though the easiest subgradient method for solving this problem.\n",
    "\n",
    "## Subgradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63944d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0068d7f",
   "metadata": {},
   "source": [
    "## Benefit of L1 regularization\n",
    "\n",
    "\n",
    "xxx resulting in more 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376ee72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10bdd978",
   "metadata": {},
   "source": [
    "This article went through the basic subgradient method in solving Lasso regression. There are other more interesting methods such as proxima gradient method, accelerate proximal gradient method, and Frank-Wolfe method. I might write more about those different methods later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
