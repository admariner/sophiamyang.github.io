{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049b25a7",
   "metadata": {},
   "source": [
    "# Multiclass logistic regression from scratch\n",
    "*Math and gradient decent implementation in Python*\n",
    "\n",
    "Video: https://youtu.be/wY3PJGZEyY4\n",
    "\n",
    "Multiclass logistic regression is also called multinomial logistic regression and softmax regression. It is used when we want to predict more than 2 classes. A lot of people use multiclass logistic regression all the time, but don't really know how it works. So, I am going to walk you through how the math works and implement it using gradient descent from scratch in Python. \n",
    "\n",
    "Disclaimer: there are various notations on this topic. I am using the notation that I think is easy to understand and visualize. You may find other notations in other places such as matrices and vectors being transposed.\n",
    "\n",
    "## Problem statement \n",
    "Let's assume we have N people/observations, each person has M features, and they belong to C classes. We are given: \n",
    "- A matrix $X$ is $\\mathbb{R}^{N\\times M}$. $X_{ij}$ represents person i with feature j.\n",
    "- A vector $Y$ is $\\mathbb{R}^{N}$. $Y_{i}$ represents person i belonging to class k.\n",
    "\n",
    "We do not know:\n",
    "- The weight matrix $W$ is $\\mathbb{R}^{M\\times C}$.$W_{jk}$ represents the weights for feature j and class k.\n",
    "\n",
    "We want to figure out $W$ and use $W$ to predict the class membership of any given observation X. \n",
    "\n",
    "## Multiclass logistic regression workflow\n",
    "If we know $X$ and $W$ (let's say we give $W$ initial values of all 0s for example), Figure 1 shows the workflow of multiclass logistic regression forward path. \n",
    "- First, we calculate the product of $X$ and $W$, here we let $Z = -XW$. \n",
    "    - Sometimes people don't include a negative sign here. It doesn't matter if there is a negative sign here or not. \n",
    "    - Sometimes we would also add a bias term. For simplicity, let's only look at the weights in this article.\n",
    "- Second, we take the softmax for each row $Z_{i}$: $P_{i} = $softmax$(Z_{i}) = \\frac{exp(Z_{i})}{\\sum_{k=0}^{C} exp(Z_{ik})}$.Each row of $Z_{i}$ should be the product of each row of $X$ (i.e., $X_{i}$) and the entire matrix of $W$. Now each row of $P$ should add up to 1.\n",
    "- Third, we take the argmax for each row and find the class with the highest probability.\n",
    "\n",
    "\n",
    "![](f1.png)\n",
    "*Figure 1. Multiclass logistic regression forward path.*\n",
    "\n",
    "<!-- <figure>\n",
    "  <img src=\"f1.png\" width=\"600\">\n",
    "    <figcaption><i>Figure 1. Multiclass logistic regression forward path.</i></figcaption>\n",
    "</figure>\n",
    " -->\n",
    "Figure 2 shows another view of the multiclass logistic regression forward path when we only look at one observation at a time:\n",
    "- First, we calculate the product of $X_i$ and W, here we let $Z_i = -X_iW$. \n",
    "- Second, we take the softmax for this row $Z_{i}$: $P_{i} = $softmax$(Z_{i}) = \\frac{exp(Z_{i})}{\\sum_{k=0}^{C} exp(Z_{ik})}$.\n",
    "- Third, we take the argmax for this row $P_{i}$ and find the index with the highest probability as $Y_i$.\n",
    "\n",
    "\n",
    "![](f2.png)\n",
    "*Figure 2. Operation on one row.*\n",
    "<!-- <figure>\n",
    "  <img src=\"f2.png\" width=\"600\">\n",
    "    <figcaption><i>Figure 2. Operation on one row.</i></figcaption>\n",
    "</figure>\n",
    " -->\n",
    "### Likelihood\n",
    "\n",
    "Recall that in the problem statement that we said we are given $Y$. So for a given observation, we know the class of this observation, which is $Y_i$. The likelihood function of $Y_i$ given $X_i$ and $W$ is the probability of observation i and class $k=Y_i$, which is the softmax of $Z_{i, k=Y_i}$. And the likelihood function of $Y$ given $X$ and $W$ is the product of all the observations. Figure 3 helps us understand this process from $Y_i$ trace backward to $W_{k=Y_i}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- $p(Y_i|X_i, W) = P_{i, k=Y_i} = $ softmax$(Z_{i, k=Y_i}) = \\frac{exp(Z_{i,k=Y_i})}{\\sum_{k=0}^{C} exp(Z_{ik})} = \\frac{\\exp(-X_{i}W_{k=Y_i})}{\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})}$\n",
    "- $p(Y|X, W) = \\prod_{i=1}^{N}\\frac{\\exp(-X_{i}W_{k=Y_i})}{\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})} $\n",
    "\n",
    "\n",
    "![](f3.png)\n",
    "*Figure 3. Calculate likihood.*\n",
    "<!-- <figure>\n",
    "  <img src=\"f3.png\" width=\"600\">\n",
    "    <figcaption><i>Figure 3. Calculate likihood.</i></figcaption>\n",
    "</figure>\n",
    " -->\n",
    "\n",
    "### Loss function / negative log-likelihood:\n",
    "\n",
    "Next, we calculate the loss function. We use the negative log-likelihood function and normalized it by the sample size. One thing to note here is that $W_{k=Y_i} = WY^T_{i(onehot\\_encoded)}$ and $\\sum_{i=1}^{N}X_iW_{k=Y_i} = Tr(XWY^T_{onehot\\_encoded})$. $Tr$ means the sum of elements on the main diagonal.\n",
    "\n",
    "\n",
    "![](f5.png)\n",
    "*Figure 4. Matrix calculations.*\n",
    "<!-- <figure>\n",
    "  <img src=\"f5.png\" width=\"800\">\n",
    "    <figcaption><i>Figure 4. Matrix calculations.</i></figcaption>\n",
    "</figure>\n",
    "\n",
    " -->\n",
    "\n",
    "$l(W) $\n",
    "\n",
    "$= -\\frac{1}{N}\\log p(Y|X, W) $\n",
    "\n",
    "$= \\frac{1}{N}(\\sum_{i=1}^{N}(X_iW_{k=Y_i} + \\log {\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})})) $\n",
    "\n",
    "$= \\frac{1}{N}(\\sum_{i=1}^{N}(X_iW_{k=Y_i} + \\sum_{i=1}^{N}\\log {\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})}) $\n",
    "\n",
    "To write loss in matrix form: \n",
    "\n",
    "$l(W) $\n",
    "\n",
    "$= \\frac{1}{N}(\\sum_{i=1}^{N}(X_iWY^T_{i(onehot\\_encoded)}) + \\sum_{i=1}^{N}\\log {\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})}) $\n",
    "\n",
    "$= \\frac{1}{N}(Tr(XWY^T_{onehot\\_encoded}) + \\sum_{i=1}^{N}\\log {\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})}) $\n",
    "\n",
    "\n",
    "$= \\frac{1}{N}(Tr(XWY^T_{onehot\\_encoded}) + \\sum_{i=1}^{N}\\log {\\sum_{k=0}^{C} \\exp((-XW)_{ik})} $\n",
    "\n",
    "\n",
    "We often add an $l^2$ regularization term to the loss function and try to minimize the combined function. In fact, the default of scikit-learn uses $l^2$ penalities. $l^1$ regularization is also very commonly used. Here we use the $l^2$  regularization.\n",
    "\n",
    "\n",
    "\n",
    "$f(W) $\n",
    "\n",
    "$=$ loss + regularization \n",
    "\n",
    "$= \\frac{1}{N}\\sum_{i=1}^{N}(X_iW_{k=Y_i} + \\log {\\sum_{k=0}^{C} \\exp(-X_{i}W_{k})}) + \\mu ||W||^2 $\n",
    "\n",
    "\n",
    "\n",
    "### Gradient:\n",
    "\n",
    "The gradient calculation is as follows. One thing to note that the gradient of $W_{k=Y_i}$ with respect to $W_k$ is the identity matrix $I_{[Y_i=k]}$.\n",
    "\n",
    "$\\nabla_{W_{k}} f(W) $\n",
    "\n",
    "$= \\frac{1}{N}\\sum_{i=1}^{N}(X_i^TI_{[Y_i=k]} - X_i^T\\frac{\\exp(-X_iW_k)}{\\sum_{k=0}^{C}\\exp(-X_iW_k)}) + 2\\mu W $\n",
    "\n",
    "$= \\frac{1}{N}\\sum_{i=1}^{N}(X_i^TI_{[Y_i=k]} - X_i^TP_i) + 2\\mu W $\n",
    "\n",
    "$= \\frac{1}{N}(\\sum_{i=1}^{N}X_i^TI_{[Y_i=k]} - \\sum_{i=1}^{N}X_i^TP_i) + 2\\mu W $\n",
    "\n",
    "$= \\frac{1}{N}(X^TY_{onehot\\_encoded} - X^TP) + 2\\mu W $\n",
    "\n",
    "$= \\frac{1}{N}(X^T(Y_{onehot\\_encoded} - P)) + 2\\mu W $\n",
    "\n",
    "### Gradient Descent Implementation\n",
    "\n",
    "Now we have calculated the loss function and the gradient function. We can implement the loss and gradient functions in Python, and implement a very basic gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b035ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.special import softmax\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def loss(X, Y, W):\n",
    "    \"\"\"\n",
    "    Y: onehot encoded\n",
    "    \"\"\"\n",
    "    Z = - X @ W\n",
    "    N = X.shape[0]\n",
    "    loss = 1/N * (np.trace(X @ W @ Y.T) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))\n",
    "    return loss\n",
    "\n",
    "def gradient(X, Y, W, mu):\n",
    "    \"\"\"\n",
    "    Y: onehot encoded \n",
    "    \"\"\"\n",
    "    Z = - X @ W\n",
    "    P = softmax(Z, axis=1)\n",
    "    N = X.shape[0]\n",
    "    gd = 1/N * (X.T @ (Y - P)) + 2 * mu * W\n",
    "    return gd\n",
    "\n",
    "def gradient_descent(X, Y, max_iter=1000, eta=0.1, mu=0.01):\n",
    "    \"\"\"\n",
    "    Very basic gradient descent algorithm with fixed eta and mu\n",
    "    \"\"\"\n",
    "    Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1,1))\n",
    "    W = np.zeros((X.shape[1], Y_onehot.shape[1]))\n",
    "    step = 0\n",
    "    step_lst = [] \n",
    "    loss_lst = []\n",
    "    W_lst = []\n",
    " \n",
    "    while step < max_iter:\n",
    "        step += 1\n",
    "        W -= eta * gradient(X, Y_onehot, W, mu)\n",
    "        step_lst.append(step)\n",
    "        W_lst.append(W)\n",
    "        loss_lst.append(loss(X, Y_onehot, W))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'step': step_lst, \n",
    "        'loss': loss_lst\n",
    "    })\n",
    "    return df, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba557da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass:\n",
    "    def fit(self, X, Y):\n",
    "        self.loss_steps, self.W = gradient_descent(X, Y)\n",
    "\n",
    "    def loss_plot(self):\n",
    "        return self.loss_steps.plot(\n",
    "            x='step', \n",
    "            y='loss',\n",
    "            xlabel='step',\n",
    "            ylabel='loss'\n",
    "        )\n",
    "\n",
    "    def predict(self, H):\n",
    "        Z = - H @ self.W\n",
    "        P = softmax(Z, axis=1)\n",
    "        return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38439c4b",
   "metadata": {},
   "source": [
    "Next, we try our model on the iris dataset. We fit the model and then plot the loss against the steps, we see that our loss function goes down over time. When we look at the prediction of our data, we see that the algorithm predicts most of the classes correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6c748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_iris().data\n",
    "Y = load_iris().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23810a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Multiclass()\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2efc7f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='step', ylabel='loss'>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAi8UlEQVR4nO3de3hdZZn38e+9D0mapueGFtpCCxSxgCCEAoIFAaGAY2VQpF5AQZTBGRgZZ9DyIuOBd8ZDx/E0IPJqUUaBMoIOAlKUcUAUoWlpoQWKoRyattC0pcc0zel+/1gr6U6yd5u0WXsle/0+l/vKWs+z9s79pJhf1vExd0dERJIrFXcBIiISLwWBiEjCKQhERBJOQSAiknAKAhGRhMvEXUBfjR071idPnhx3GSIig8rixYs3uHt1vr5BFwSTJ0+mtrY27jJERAYVM3ujUJ8ODYmIJJyCQEQk4RQEIiIJN+jOEYiI9IeWlhbq6+tpamqKu5R+VVFRwcSJE8lms71+j4JARBKpvr6eYcOGMXnyZMws7nL6hbuzceNG6uvrmTJlSq/fp0NDIpJITU1NjBkzpmRCAMDMGDNmTJ/3chQEIpJYpRQCHfZlTIkJgpVvbeNbj61k4/ZdcZciIjKgJCYIXm3Yzvf/p44N25vjLkVEBICqqqq4SwASFATZdDDUlrb2mCsRERlYEhMEZZlgqM0KAhEZYNydG264gaOPPppjjjmGBQsWALBu3TpmzJjBcccdx9FHH80f/vAH2trauOKKKzq3/fa3v73f3z8xl49m08EJlJZWBYGIdPWVX6/gxbVb+/Uzpx00nC/91VG92vaBBx5g6dKlLFu2jA0bNnDiiScyY8YM7r77bs4991xuuukm2traaGxsZOnSpaxZs4bly5cDsHnz5v2uNTl7BJ2HhjRHs4gMLE899RSzZ88mnU4zbtw4Tj/9dBYtWsSJJ57InXfeyZe//GVeeOEFhg0bxqGHHsqqVau47rrrePTRRxk+fPh+f//I9gjMbD7wIWC9ux+dp9+A7wLnA43AFe6+JKp6dI5ARArp7V/uUXHP/wfqjBkzePLJJ3n44Ye57LLLuOGGG7j88stZtmwZCxcu5NZbb+W+++5j/vz5+/X9o9wj+Akwcw/95wFTw9fVwA8irKUzCHSOQEQGmhkzZrBgwQLa2tpoaGjgySefZPr06bzxxhsccMABfPrTn+aqq65iyZIlbNiwgfb2di666CJuueUWlizZ/7+fI9sjcPcnzWzyHjaZBdzlQRT+2cxGmtmB7r4uinrKMuE5AgWBiAwwF154IU8//TTHHnssZsY3v/lNxo8fz09/+lPmzZtHNpulqqqKu+66izVr1nDllVfS3h78Lvva1762398/zpPFE4DVOev1YVuPIDCzqwn2Gjj44IP36Zvp0JCIDDTbt28HgruB582bx7x587r0z5kzhzlz5vR4X3/sBeSK82Rxvvug8x4oc/c73L3G3Wuqq/POtLZXnYeGdNWQiEgXcQZBPTApZ30isDaqb7b7HIGuGhIRyRVnEDwIXG6Bk4EtUZ0fgJzLR7VHICKhQlfrDGb7MqYoLx+9BzgDGGtm9cCXgCyAu98OPEJw6WgdweWjV0ZVC0BWJ4tFJEdFRQUbN24sqUdRd8xHUFFR0af3RXnV0Oy99Dvwd1F9/+50slhEck2cOJH6+noaGhriLqVfdcxQ1heJecREJhUkvs4RiAhANpvt0yxepSwxj5gwM8rSKe0RiIh0k5gggODBczpZLCLSVbKCIKM9AhGR7pIVBOmUzhGIiHSTqCDQOQIRkZ4SFQTZtCkIRES6SVgQpPSsIRGRbhIXBNojEBHpKllBkNHJYhGR7hIVBOXplO4jEBHpJlFBkM2YpqoUEekmUUFQnkmzq7Ut7jJERAaURAVBZVmaxl0KAhGRXMkLgmYFgYhIroQFQYYdza1xlyEiMqAkKgiGlgd7BKU4PZ2IyL6KNAjMbKaZrTSzOjObm6d/lJn90syeN7NnzezoKOupLMvQ1u66ckhEJEdkQWBmaeBW4DxgGjDbzKZ12+z/AEvd/T3A5cB3o6oHgnMEgE4Yi4jkiHKPYDpQ5+6r3L0ZuBeY1W2bacDjAO7+MjDZzMZFVVBnELQoCEREOkQZBBOA1Tnr9WFbrmXAXwOY2XTgEKDHrMtmdrWZ1ZpZ7f5MNF1ZFkzR3LhLJ4xFRDpEGQSWp637WdqvA6PMbClwHfAc0OO3tLvf4e417l5TXV29zwUNLQ/2CHboElIRkU6ZCD+7HpiUsz4RWJu7gbtvBa4EMDMDXgtfkRiSDfcIdAmpiEinKPcIFgFTzWyKmZUBlwAP5m5gZiPDPoBPAU+G4RCJjj0CnSwWEdktsj0Cd281s2uBhUAamO/uK8zsmrD/duDdwF1m1ga8CFwVVT2gk8UiIvlEeWgId38EeKRb2+05y08DU6OsIVdVeRaAl9dt5fQjqhkxJFusby0iMmAl6s7isVVlpAxu+99X+evb/hh3OSIiA0KigiCTTtEeXrf0asOOeIsRERkgEhUEIiLSU+KC4MFrT+1c1iQ1IiIJDIL3TBzZufzQsnXxFSIiMkAkLghylWcTPXwRESDhQdBxX4GISJIlMgiOnjAcgJY2TVAjIpLIIPj2xccB0NyqCWpERBIZBOWZ4JDQw8/rZLGISCKDoCwTDPvRFW/FXImISPwSGQTlmUQOW0Qkr0T+RixTEIiIdErkb0QFgYjIbon8jZhJ5ZtFU0QkmRIZBGbGEeOqAFiw6M2YqxERiVekQWBmM81spZnVmdncPP0jzOzXZrbMzFaY2ZVR1pPrsOogCH74xKpifUsRkQEpsiAwszRwK3AeMA2YbWbTum32d8CL7n4scAbwrZw5jIsipcNEIpJwUe4RTAfq3H2VuzcD9wKzum3jwDAzM6AK2AS0RliTiIh0E2UQTABW56zXh225/oNgAvu1wAvAZ929x3MfzOxqM6s1s9qGhoZ+Ka7jOUObG5v75fNERAarKIMg3zGX7k95OxdYChwEHAf8h5kN7/Em9zvcvcbda6qrq/uluOa2IG82bG9mZ7MmqBGR5IoyCOqBSTnrEwn+8s91JfCAB+qA14AjI6yp05CcuQiaWhQEIpJcUQbBImCqmU0JTwBfAjzYbZs3gbMAzGwc8C6gKJfx/MuFx3Qut7brcdQiklyZqD7Y3VvN7FpgIZAG5rv7CjO7Juy/HbgF+ImZvUBwKOkL7r4hqppyja0q71xuadPjqEUkuSILAgB3fwR4pFvb7TnLa4FzoqyhN6675znu/8z74i5DRCQWibyzuLvFb7wTdwkiIrFREIiIJJyCIOSuE8YikkyJDoILjjmwc/nBZd2vbBURSYZEB8Hs6Qd3Ltet3x5jJSIi8Ul0ELTlHA4KHnckIpI8iQ6C9pwbyb73+F9irEREJD6JDgLdUSwikvAgOG7SyLhLEBGJXaKDoHpYeZf1+U+9FlMlIiLxSXQQdPdvj62MuwQRkaJTEORo1LwEIpJAiQ+CEUOyXdbXb22KqRIRkXgkPggevf79XdbfaWyJqRIRkXgkPggOHDGky/rjL78dUyUiIvFIfBB0981HdcJYRJIl0iAws5lmttLM6sxsbp7+G8xsafhabmZtZjY6ypp6Q08iFZEkiSwIzCwN3AqcB0wDZpvZtNxt3H2eux/n7scBNwJPuPumqGoq5IhxVV3Wf/qn14tdgohIbKLcI5gO1Ln7KndvBu4FZu1h+9nAPRHWU9C7DxzeZf2lddviKENEJBZRBsEEYHXOen3Y1oOZVQIzgfsL9F9tZrVmVtvQ0NDvhXY/ErSgdnX+DUVESlCUQZDvuc6FDr7/FfDHQoeF3P0Od69x95rq6up+K7DDZ844rN8/U0RksIgyCOqBSTnrE4FC04BdQkyHhaDnoSHQc4dEJDmiDIJFwFQzm2JmZQS/7B/svpGZjQBOB/47wlr67KsPvRh3CSIiRRFZELh7K3AtsBB4CbjP3VeY2TVmdk3OphcCj7n7jqhq6Y3KsnSPti26y1hEEiDS+wjc/RF3P8LdD3P3fwnbbnf323O2+Ym7XxJlHb1x96dP7tH2/JrNxS9ERKTIdGdxKN9NZJf9+NkYKhERKS4FQehd44flbdddxiJS6hQEocqyDJbngtcFi3RPgYiUtl4FgZl91syGW+DHZrbEzM6JurhiO3h0ZY+27z3+lxgqEREpnt7uEXzS3bcC5wDVwJXA1yOrKiYLrj6lR9vaLZqoRkRKW2+DoOOgyfnAne6+jPx3Dg9q40dU5G1fvmZLkSsRESme3gbBYjN7jCAIFprZMKA9urIGlg99/6m4SxARiUxvg+AqYC5wors3AlmCw0Ml55OnTsnb/pYOEYlIieptEJwCrHT3zWZ2KfBFoCSPl0yfMipv+/1L6otciYhIcfQ2CH4ANJrZscDngTeAuyKrKkYzjz4wb/u8hZrCUkRKU2+DoNWDO6tmAd919+8C+e/AKmGvb4j1cUgiIpHobRBsM7MbgcuAh8NpKLPRlRWvX197Wt72OXfqkRMiUnp6GwQfB3YR3E/wFsFMY/MiqypmE0cNydv+xsZG2tr1yAkRKS29CoLwl//PgRFm9iGgyd1L8hwBwKihZQX7/li3oYiViIhEr7ePmLgYeBb4GHAx8IyZfTTKwuJ25amT87ZfPl+Hh0SktPT20NBNBPcQzHH3y4HpwM3RlRW/L14wrWDf069uLGIlIiLR6m0QpNx9fc76xj68d1BKpwo/QeOf/mtZESsREYlWb3+ZP2pmC83sCjO7AngYeGRvbzKzmWa20szqzGxugW3OMLOlZrbCzJ7ofenRu+n8d+dtX7N5J00tbUWuRkQkGr09WXwDcAfwHuBY4A53/8Ke3hNeYnorcB4wDZhtZtO6bTMSuA34sLsfRXAOYsC49ORDCvbNvf/5IlYiIhKdXh/ecff73f1z7v4P7v7LXrxlOlDn7qvcvRm4l+CGtFyfAB5w9zfD77GeAWRIWZpCR4h+tXQtza2Jee6eiJSwPQaBmW0zs615XtvMbOtePnsCkDu9V33YlusIYJSZ/a+ZLTazywvUcbWZ1ZpZbUNDw97G1K+e+sKZBft0KamIlII9BoG7D3P34Xlew9x9+F4+O9/f0t3vxsoAJwAXAOcCN5vZEXnquMPda9y9prq6ei/ftn+NH55/jgKAK3+yqIiViIhEI8orf+qBSTnrE4G1ebZ51N13uPsG4EmCcxADRiplfPjYgwr2/8/LbxexGhGR/hdlECwCpprZFDMrAy4BHuy2zX8D7zezjJlVAicBL0VY0z75+kXHFOz75E9qi1iJiEj/y0T1we7eambXAguBNDDf3VeY2TVh/+3u/pKZPQo8TzDj2Y/cfXlUNe2ryrIMKYNCjxlav7WJA/ZwCElEZCCz4OnSg0dNTY3X1hb/r/BnX9vExT98Om/fmKFlLL75g0WuSESk98xssbvX5Osr6buD+9P0KaML9m3c0aypLEVk0FIQ9MH9n3lfwb7r7llSxEpERPqPgqAPTjgk/3zGAItef4ctO1uKWI2ISP9QEPTR3595eMG+OXpEtYgMQgqCPvrs2T3ud+u0dPVm2jWDmYgMMgqCPkqnjGtOP6xg/z/qEdUiMsgoCPbBZ8+aWrDvl8+tYXNjcxGrERHZPwqCfTCkLM21Hyh8ruCrv36xiNWIiOwfBcE++qdz31Ww74Hn1tDY3FrEakRE9p2CYD/8zemHFuy7Yr6eTCoig4OCYD98/twjC/Y9+/omXUEkIoOCgmA/pFPGdz5+XMH+T9+lJ5OKyMCnINhPs44rPFfB4y+vZ83mnUWsRkSk7xQE+8nMePjvTyvYf/Ht+Z9YKiIyUCgI+sFRB40o2Ldm807q32ksYjUiIn2jIOgnT33hAwX7TvvG74tYiYhI30QaBGY208xWmlmdmc3N03+GmW0xs6Xh65+jrCdKE0dV8omTDi7YX7d+exGrERHpvciCwMzSwK3AecA0YLaZTcuz6R/c/bjw9dWo6imGr3z4qIJ9Z//7E0WsRESk96LcI5gO1Ln7KndvBu4FZkX4/WKXTae49RPHF+xfsOjNIlYjItI7UQbBBGB1znp92NbdKWa2zMx+Y2Z5/6Q2s6vNrNbMahsaGqKotd9c8J4DC/Z94f4XGGxzRItI6YsyCCxPW/ffgkuAQ9z9WOD7wK/yfZC73+HuNe5eU11d3b9VRmBPJ47nLVxZxEpERPYuyiCoByblrE8E1uZu4O5b3X17uPwIkDWzsRHWVBQTR1VyQ4GH0t32v6+yfZceSCciA0eUQbAImGpmU8ysDLgEeDB3AzMbb2YWLk8P69kYYU1F87dnFJ685tIfPVPESkRE9iyyIHD3VuBaYCHwEnCfu68ws2vM7Jpws48Cy81sGfA94BIvkYPoZsaj178/b9/S1Zt5ce3WIlckIpKfDbbfuzU1NV5bO3ge5jZn/rM88Ur+E9yvf/2CIlcjIkllZovdvSZfn+4sjtiP5uT9uQPwmxfWFbESEZH8FAQRy6ZTPPYPM/L2febnS2hqaStyRSIiXSkIiuCIccM4esLwvH3X3v1ckasREelKQVAkv/zbU/O2/+6lt3m1Qc8hEpH4KAiKJJtOFZy34Kxv6TlEIhIfBUERHXXQiIL3F/zquTVFrkZEJKAgKLLPz8w/4f31C5byzo7mIlcjIqIgiMVzN38wb/v0f/1dkSsREVEQxGLU0DLuuOyEHu0tbc5vX3w7hopEJMkUBDE556jxHDl+WI/2T99VS1v74LrbW0QGNwVBjB66Lv9VROd998kiVyIiSaYgiFEmnWLRTWf3aH/l7e089PzaPO8QEel/CoKYVQ8r52dXndSj/dq7n2OH5i0QkSJQEAwAp00dyymHjunRftSXFsZQjYgkjYJggPj5p3ruFQA8tuKtIlciIkmjIBggUinjpa/O7NF+9X8u1o1mIhKpSIPAzGaa2UozqzOzuXvY7kQzazOzj0ZZz0A3pCzNEzec0aP9vbf8tvjFiEhiRBYEZpYGbgXOA6YBs81sWoHtvkEwpWXiHTJmKN/62LE92ufe/3wM1YhIEkS5RzAdqHP3Ve7eDNwLzMqz3XXA/cD6CGsZVC46YSKXnnxwl7Z7F61m0eubYqpIREpZlEEwAVids14ftnUyswnAhcDte/ogM7vazGrNrLahIf/8v6Xm/37kGIaVZ7q0fez2p9nW1BJTRSJSqqIMAsvT1v3ZCd8BvuDue5yv0d3vcPcad6+prq7ur/oGvKVfOqdH2zFffiyGSkSklEUZBPXApJz1iUD322VrgHvN7HXgo8BtZvaRCGsaVNIpY8VXzu3RfuMDL8RQjYiUqiiDYBEw1cymmFkZcAnwYO4G7j7F3Se7+2TgF8DfuvuvIqxp0BlanuGPc8/s0nbPs2/y8PPrYqpIREpNZEHg7q3AtQRXA70E3OfuK8zsGjO7JqrvW4omjBzC7z53epe2v7t7Ceu27IypIhEpJeY+uB55XFNT47W1tXGXEYvHX3qbq37adewv3zKTimw6popEZLAws8XuXpOvT3cWDyJnvXscP+w2oc2RNz9Ku+YvEJH9oCAYZM49ajxf+fBRXdpm/78/M9j27ERk4FAQDEJz3jeZ6848vHP9mdc2Mfd+XUkkIvtGQTBI/eM57+LmD+1+YseC2tX85gVdSSQifacgGMSuOm0KfzPj0M71z/x8Cb9eppnNRKRvFASD3I3nv5t/OueIzvXr7nmO5958J8aKRGSwURCUgGvPnMo3P/qezvULb/sTSxQGItJLCoIScXHNJL7z8eM61//6tj/x7Gt6WqmI7J2CoIR85L0TuDtnysuLf/g0i9/QnoGI7JmCoMS87/CxXWY5u+gHf+LR5Zr3WEQKUxCUoEPGDOX5L+9+hPU1P1vMrb+vi7EiERnIFAQlanhFllf/9XxGDMkCMG/hSubMf1Z3IItIDwqCEpZOGUv/+YNcddoUAJ54pYEpNz5Cc2t7zJWJyECiIChxZsbNH5rGgqtP7mw74ou/4c2NjTFWJSIDiYIgIU46dAwvfnX3bGcz5v2e7z/+lxgrEpGBQkGQIJVlGV7/+gX8ffjAum/99hVO+tffsX5bU8yViUicIg0CM5tpZivNrM7M5ubpn2Vmz5vZUjOrNbPToqxHAp875138ae6ZVGRTvL11F9P/5XHuevp1nUgWSajIZigzszTwCvBBgonsFwGz3f3FnG2qgB3u7mb2HoLpLI/c0+cmeYayKNz5x9e45aEXaXc4cvwwvvLhozjp0DFxlyUi/SyuGcqmA3Xuvsrdm4F7gVm5G7j7dt+dREMB/UlaZFeeOoWXbzmP68+eyqqGHXz8jj9z2Y+f0eMpRBIkyiCYAKzOWa8P27owswvN7GXgYeCTEdYjBZRlUlx/9hEsuulsrjn9MJ57czMX//BpLvjeH/jF4noam1vjLlFEIhTloaGPAee6+6fC9cuA6e5+XYHtZwD/7O5n5+m7Grga4OCDDz7hjTfeiKRmCWxrauH+xfX8/Jk3+cv67QzJpjnnqHGcevhYTpoymoNHV2JmcZcpIn2wp0NDUQbBKcCX3f3ccP1GAHf/2h7e8xpwortvKLSNzhEUj7vz51WbeHDZGhaueJtNO5oBGD+8gulTRjN9ymhOPnQ0h1VXKRhEBri4giBDcLL4LGANwcniT7j7ipxtDgdeDU8WHw/8GpjoeyhKQRAPd6du/XaeeW1T8Fq1kfXbdgEwZmgZJ04ezZEHDuPwA6o4rLqKKWOHUpFNx1y1iHTYUxBkovqm7t5qZtcCC4E0MN/dV5jZNWH/7cBFwOVm1gLsBD6+pxCQ+JgZU8cNY+q4YVx68iG4O29uauSZVUEwLH5jEwtffIuOf72UwaTRlRxWXcXBoyuZNLqSSaOGcPCYSiaNqmRoeWT/6YlIH0W2RxAV7REMXE0tbaxq2EFdw3bq1m/n1fXbebVhO6s3NbKjua3LtmOGlgXhMLqSg0ZUMH5EBQeOqGD8iCEcOKKCsVXlpFM63CTSX2LZI5DkqcimmXbQcKYdNLxLu7vzTmMLqzc18mb4qn8n+Lps9WYWLm+iua3rg/DSKWPcsPIwIIYwfkQFBwwrZ0xVOWOryhhbVU71sHJGDy0jm9YN8iL7Q0EgkTMzRg8tY/TQMo6dNLJHv7uzaUcz67Y08daWJt7aGnxdt6WJt7bu5KW3tvL7letp7LZX0WFUZbZLQHSExMjKLCOHlAVfK7OMrCxj5JAslWVpndwWyaEgkNiZGWOqgr/2j54wIu827s6O5jY2bNvFhu272LC9OfwavrYF68vXbGHj9ma27Sp870NZOsWIyiwjh2QZVVnWuTyyMsvwiixVFRmqyjMMq8gyrCLDsHC9qiLD8Ios5ZmUgkRKioJABgUzC34Zl2eYPHboXrdvamljy84WNje28E5jM5sbW9iyszlcz11uZvWmRpbvDJabWvY+V0MmZUE4VGSoKg/DojzDkLI0lWVpKsvC5Ww6bMtQWZbO6U8zJJvZvRxuo3MiEhcFgZSkimyaimyaccMr+vS+lrZ2tje1sn1XK9uaWtnW1LJ7eVcr27u3NbWyfVcLb21tYmdzG43NbTQ2t7KzpY2Wtr5diFGWSTEkm6Y8k6I8m6Iik6Y8m6I8E7ZlUlR09Hf2BcsV2fxtZZkU2XTuyzqXy9IpMuF6WTpFNmNkUsE22uNJFgWBSI5sOsWooWWMGlq235/V0tZOY3NbGBCtYUiEQdGx3NLGzrBvZ3MbTS1t7GptD19t7Gpppyn8uq2pNWhrbd+9Xdjf3xf/dQRGJmVdwiSTtiA0ckIkkzLSKQu/7l7f3dZ9PXhvyqzLe1OpruvpdIp07jbd3pNOBetmdFlOWUdfsCeZMiNdsC//ezuWO9+bosfndLy3FEJTQSASkWw6xYghqc55o6Pi7rS2e7eA2B0oLW0dL6elY709Z7mjrxfLzW3ttOa0N7cGr9Z2p92d1janrd1pbW+n3aG1vZ22tqC+tnanzb3nevvguoQ9n1RHOIQhkzLDCL4S/I9UanebhUFk0BkswfLuoLGO94ULKTMuOXESn3r/of1ev4JAZJAzs86/4KsG4Y167h3hsYewaAvCJXe7dnfaHdraHQ+X291pb9+93OZhX3vYl7udE26b/70dde3ePrePLjXkbtux7ICH23eMs93B6dguaPN8bez+Ph3LOIytKo/k32Dw/VcjIiXFLDjsk9ETSWKjO3FERBJOQSAiknAKAhGRhFMQiIgknIJARCThFAQiIgmnIBARSTgFgYhIwg26GcrMrAF4Yx/fPhbY0I/lDAYaczJozMmwP2M+xN2r83UMuiDYH2ZWW2iqtlKlMSeDxpwMUY1Zh4ZERBJOQSAiknBJC4I74i4gBhpzMmjMyRDJmBN1jkBERHpK2h6BiIh0oyAQEUm4xASBmc00s5VmVmdmc+Oupz+Y2SQz+72ZvWRmK8zss2H7aDP7rZn9Jfw6Kuc9N4Y/g5Vmdm581e8fM0ub2XNm9lC4XtJjNrORZvYLM3s5/Pc+JQFj/ofwv+vlZnaPmVWU2pjNbL6ZrTez5TltfR6jmZ1gZi+Efd+zvk6k7OGUbKX8AtLAq8ChQBmwDJgWd139MK4DgePD5WHAK8A04JvA3LB9LvCNcHlaOPZyYEr4M0nHPY59HPvngLuBh8L1kh4z8FPgU+FyGTCylMcMTABeA4aE6/cBV5TamIEZwPHA8py2Po8ReBY4hWCa498A5/WljqTsEUwH6tx9lbs3A/cCs2Kuab+5+zp3XxIubwNeIvg/0CyCXxyEXz8SLs8C7nX3Xe7+GlBH8LMZVMxsInAB8KOc5pIds5kNJ/iF8WMAd292982U8JhDGWCImWWASmAtJTZmd38S2NStuU9jNLMDgeHu/rQHqXBXznt6JSlBMAFYnbNeH7aVDDObDLwXeAYY5+7rIAgL4IBws1L5OXwH+DzQntNWymM+FGgA7gwPh/3IzIZSwmN29zXAvwFvAuuALe7+GCU85hx9HeOEcLl7e68lJQjyHS8rmetmzawKuB+43t237mnTPG2D6udgZh8C1rv74t6+JU/boBozwV/GxwM/cPf3AjsIDhkUMujHHB4Xn0VwCOQgYKiZXbqnt+RpG1Rj7oVCY9zvsSclCOqBSTnrEwl2Mwc9M8sShMDP3f2BsPntcHeR8Ov6sL0Ufg6nAh82s9cJDvGdaWY/o7THXA/Uu/sz4fovCIKhlMd8NvCauze4ewvwAPA+SnvMHfo6xvpwuXt7ryUlCBYBU81sipmVAZcAD8Zc034Lrwz4MfCSu/97TteDwJxweQ7w3zntl5hZuZlNAaYSnGQaNNz9Rnef6O6TCf4d/8fdL6W0x/wWsNrM3hU2nQW8SAmPmeCQ0MlmVhn+d34WwTmwUh5zhz6NMTx8tM3MTg5/VpfnvKd34j5rXsSz8+cTXFXzKnBT3PX005hOI9gFfB5YGr7OB8YAjwN/Cb+OznnPTeHPYCV9vLJgoL2AM9h91VBJjxk4DqgN/61/BYxKwJi/ArwMLAf+k+BqmZIaM3APwTmQFoK/7K/alzECNeHP6VXgPwifGtHblx4xISKScEk5NCQiIgUoCEREEk5BICKScAoCEZGEUxCIiCScgkBkH5jZ9WZWGXcdIv1Bl4+K7IPwzuYad98Qdy0i+ysTdwEiA134gLf7CG7dTwP/RfD8m9+b2QZ3/4CZnUNwA1Q5wU09V7r79jAwFgAfCD/uE+5eV+wxiOyJDg2J7N1MYK27H+vuRxM8/XQt8IEwBMYCXwTOdvfjCe4A/lzO+7e6+3SCOz6/U9TKRXpBQSCydy8AZ5vZN8zs/e6+pVv/yQSThvzRzJYSPB/mkJz+e3K+nhJ1sSJ9pUNDInvh7q+Y2QkEz3H6mpk91m0TA37r7rMLfUSBZZEBQXsEInthZgcBje7+M4LJUo4HthFMDwrwZ+BUMzs83L7SzI7I+YiP53x9ujhVi/Se9ghE9u4YYJ6ZtRM8JfIzBId4fmNm68LzBFcA95hZefieLxI87Rag3MyeIfjDq9Beg0hsdPmoSIR0makMBjo0JCKScNojEBFJOO0RiIgknIJARCThFAQiIgmnIBARSTgFgYhIwv1/6YnBQyfWEJEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.loss_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "57727af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0be6ab7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X) == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eecb2",
   "metadata": {},
   "source": [
    "This is a basic overview of the math and gradient descent of multiclass logistic regression. Hope you find this article helpful.\n",
    "\n",
    "By Sophia Yang on April 18, 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
